# -*- coding: utf-8 -*-
"""TRT-Engine Create.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wz4SjA_BC8x89wxYJ0NAqU0EfqXKQoln

## Optimized Trained TensorFlow Model with NVIDIA Tensor RT

In this notebook we show how to import a frozen TensorFlow graph, and use the Tensor RT to optimize it and create a runtime engine
"""

''' Import TensorRT Modules '''
import tensorrt as trt

import uff
from tensorrt.parsers import uffparser

''' Import other modules '''
import glob

G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)



# Load your newly created Tensorflow frozen model and convert it to UFF
uff_model = uff.from_tensorflow_frozen_model("keras_vgg19_frozen_graph.pb", 
                                             ["dense_2/Softmax"])

import tensorflow as tf

def printTensors(pb_file):

    # read pb into graph_def
    with tf.gfile.GFile(pb_file, "rb") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # import graph_def
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def)

    # print operations
    for op in graph.get_operations():
        print(op.name)

printTensors("keras_vgg19_frozen_graph.pb")

# Create a UFF parser to parse the UFF file created from your TF Frozen model
parser = uffparser.create_uff_parser()
parser.register_input(#input layer, (3,224,224),0) #TODO1
parser.register_output(#output layer)   #TODO2

#TODO3
#Create Engine
engine = trt.utils.uff_to_trt_engine(#logger,#model,#batch_size,1<<20,data_type)

trt.utils.write_engine_to_file("keras_vgg19_b1_FP32.engine", 
                               engine.serialize())

glob.glob("*.engine")





